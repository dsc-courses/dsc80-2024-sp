{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a44b8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from dsc80_utils import *\n",
    "import lec15_util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca51859",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 15 – Pipelines, Multicollinearity, and Generalization\n",
    "\n",
    "## DSC 80, Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22824200",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements 📣\n",
    "\n",
    "- Final Project Checkpoint 1 due **tomorrow**.\n",
    "- Lab 8 due next **Wednesday**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd851d5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda 📆\n",
    "\n",
    "- Pipelines.\n",
    "- Multicollinearity.\n",
    "- Generalization.\n",
    "    - Bias and variance.\n",
    "    - Train-test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398eb093",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question 🤔 (Answer at <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScWbVZv9hBv-wX-ItKHUVRnkPMMtfJZVfErKE9GS7_8dFcRBQ/viewform\">q.dsc80.com)</h3>\n",
    "</div>\n",
    "    \n",
    "Remember, you can always ask questions at [**q.dsc80.com**](https://docs.google.com/forms/d/e/1FAIpQLScWbVZv9hBv-wX-ItKHUVRnkPMMtfJZVfErKE9GS7_8dFcRBQ/viewform)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833fac9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f71de5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/image_0.png\" width=\"50%\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "So far, we've used transformers for feature engineering and models for prediction. We can combine these steps into a single `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1ef16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Pipeline`s in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3534d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From [`sklearn`'s documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html):\n",
    "\n",
    "> Pipeline allows you to sequentially apply a list of transformers to preprocess the data and, **if desired**, conclude the sequence with a final predictor for predictive modeling.<br><br>Intermediate steps of the pipeline must be \"transforms\", that is, they must implement `fit` and `transform` methods. The final estimator only needs to implement `fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1553e9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- General template: `pl = Pipeline([trans_1, trans_2, ..., model])`\n",
    "    - Note that the `model` is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a409a1d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once a `Pipeline` is instantiated, you can fit **all** steps (transformers and model) using `pl.fit(X, y)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170591a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To make predictions using **raw, untransformed data**, use `pl.predict(X)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508398a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The actual list we provide `Pipeline` with must be a list of **tuples**, where\n",
    "    - The first element is a \"name\" (that we choose) for the step.\n",
    "    - The second element is a transformer or estimator instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86669e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our first `Pipeline`\n",
    "\n",
    "Let's build a `Pipeline` that:\n",
    "- One hot encodes the categorical features in `tips`.\n",
    "- Fits a regression model on the one hot encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = px.data.tips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03859c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_cat = tips[['sex', 'smoker', 'day', 'time']]\n",
    "tips_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cf9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49f763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pl = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ef92c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that `pl` is instantiated, we `fit` it the same way we would fit the individual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03991a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(tips_cat, tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b902c9f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, to make predictions using **raw data**, all we need to do is use `pl.predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(tips_cat.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8518b9",
   "metadata": {},
   "source": [
    "`pl` performs **both** feature transformation and prediction with just a single call to `predict`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea45098",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can access individual \"steps\" of a `Pipeline` through the `named_steps` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c6298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.named_steps['one-hot'].transform(tips_cat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps['one-hot'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f17401",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps['lin-reg'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c7b1ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`pl` also has a `score` method, the same way a fit `LinearRegression` instance does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is this so low?\n",
    "pl.score(tips_cat, tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fc2ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More sophisticated `Pipeline`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aeac87",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the previous example, we one hot encoded every input column. **What if we want to perform different transformations on different columns?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877bd69",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Solution**: Use a `ColumnTransformer`.\n",
    "    - Instantiate a `ColumnTransformer` using a list of tuples, where:\n",
    "        - The first element is a \"name\" we choose for the transformer.\n",
    "        - The second element is a transformer instance (e.g. `OneHotEncoder()`).\n",
    "        - The third element is a **list of relevant column names**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477c359",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Planning our first `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd28cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ecc2b",
   "metadata": {},
   "source": [
    "Let's perform different transformations on the quantitative and categorical features of `tips` (note that we are not transforming `'tip'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_features = tips.drop('tip', axis=1)\n",
    "tips_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b564152",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We will leave the `'total_bill'` column untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab25c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To the `'size'` column, we will apply the `Binarizer` transformer with a threshold of 2 (big tables vs. small tables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39414c70",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To the categorical columns, we will apply the `OneHotEncoder` transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05895b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In essence, we will create a transformer that reproduces the following DataFrame:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>size</th>\n",
    "      <th>x0_Female</th>\n",
    "      <th>x0_Male</th>\n",
    "      <th>x1_No</th>\n",
    "      <th>x1_Yes</th>\n",
    "      <th>x2_Fri</th>\n",
    "      <th>x2_Sat</th>\n",
    "      <th>x2_Sun</th>\n",
    "      <th>x2_Thur</th>\n",
    "      <th>x3_Dinner</th>\n",
    "      <th>x3_Lunch</th>\n",
    "      <th>total_bill</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>16.99</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>1</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>10.34</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>1</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>21.01</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>23.68</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>1</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>24.59</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89393533",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building a `Pipeline` using a `ColumnTransformer`\n",
    "\n",
    "Let's start by creating our `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e5854",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ...\n",
    "    ],\n",
    "    remainder='...' # Specify what to do with all other columns ('total_bill' here) – drop or passthrough.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f5569",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, let's create a `Pipeline` using `preproc` as a transformer, and `fit` it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "    ...,\n",
    "    ('lin-reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(tips_features, tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5db465",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Prediction is as easy as calling `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3f595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we fit the Pipeline using tips_features, not tips_features.head()!\n",
    "pl.predict(tips_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b1a71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: `FunctionTransformer`\n",
    "\n",
    "A transformer you'll often use as part of a `ColumnTransformer` is the `FunctionTransformer`, which enables you to use your own functions on entire columns. Think of it as the `sklearn` equivalent of `apply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5dbb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FunctionTransformer(np.sqrt)\n",
    "f.transform([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69429a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 💡 Pro-Tip: Using `make_pipeline` and `make_column_transformer`\n",
    "\n",
    "Instead of using `Pipeline` and `ColumnTransformer` classes directly, `scikit-learn` provides nifty shortcut methods called `make_pipeline` and `make_column_transformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('size', Binarizer(threshold=2), ['size']),\n",
    "        ('categorical_cols', OneHotEncoder(), ['sex', 'smoker', 'day', 'time'])\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('preprocessor', preproc), \n",
    "    ('lin-reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "preproc = make_column_transformer(\n",
    "    (Binarizer(threshold=2), ['size']),\n",
    "    (OneHotEncoder(), ['sex', 'smoker', 'day', 'time']),\n",
    "    remainder='passthrough',\n",
    ")\n",
    "\n",
    "pl = make_pipeline(preproc, LinearRegression())\n",
    "# Notice that the steps in the pipeline and column transformer are\n",
    "# automatically named\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777015f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An example `Pipeline`\n",
    "\n",
    "One of the transformers we used was the `StandardScaler` transformer, which **standardizes** columns.\n",
    "\n",
    "$$z(x_i) = \\frac{x_i - \\text{mean of } x}{\\text{SD of } x}$$\n",
    "\n",
    "Let's build a `Pipeline` that:\n",
    "- Takes in the `'total_bill'` and `'size'` features of `tips`.\n",
    "- Standardizes those features.\n",
    "- Uses the resulting standardized features to fit a linear model that predicts `'tip'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7799cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define these once, since we'll use them repeatedly.\n",
    "X = tips[['total_bill', 'size']]\n",
    "y = tips['tip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model_with_std = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearRegression(),\n",
    ")\n",
    "\n",
    "\n",
    "model_with_std.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20184d8a",
   "metadata": {},
   "source": [
    "How well does our model do? We can compute its $R^2$ and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75de59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_with_std.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y, model_with_std.predict(X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395de73c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Does this model perform any better than one that _doesn't_ standardize its features? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_std = LinearRegression()\n",
    "model_without_std.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_std.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348cfbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, model_without_std.predict(X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37078e9f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**No!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac9790",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The purpose of standardizing features\n",
    "\n",
    "If you're performing \"vanilla\" linear regression – that is, using the `LinearRegression` object – then standardizing your features **will not** change your model's error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fdf45d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are other models where standardizing your features _will_ improve performance, because the methods assume features are standardized.\n",
    "    - Regularized linear regression (see [DSC 140A](https://dsc140a.com)).\n",
    "    - PCA (assumes centered data, not necessarily standardized: see [DSC 140B](https://dsc140b.com)).\n",
    "    - Clustering algorithms, e.g. $k$-means clustering (saw in DSC 40A!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede8a13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There _is_ a benefit to standardizing features when performing vanilla linear regression, as we saw in DSC 40A: the features are brought to the same scale, so the coefficients can be compared directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total bill, table size.\n",
    "model_without_std.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec928373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total bill, table size.\n",
    "model_with_std.named_steps['linearregression'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29e43e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: `Pipeline`s of just transformers\n",
    "\n",
    "If you want to apply multiple transformations to the same column in a dataset, you can create a `Pipeline` just for that column.\n",
    "\n",
    "For example, suppose we want to:\n",
    "- One hot encode the `'sex'`, `'smoker'`, and `'time'` columns.\n",
    "- One hot encode the `'day'` column, but as either `'Weekday'`, `'Sat'`, or `'Sun'`.\n",
    "- Binarize the `'size'` column.\n",
    "\n",
    "Here's how we might do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5506e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(s):\n",
    "    # The input to is_weekend is a Series!\n",
    "    return s.replace({'Thur': 'Weekday', 'Fri': 'Weekday'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_day = make_pipeline(\n",
    "    FunctionTransformer(is_weekend),\n",
    "    OneHotEncoder(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beadd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_trans = make_column_transformer(\n",
    "    (pl_day, ['day']),\n",
    "    (OneHotEncoder(drop='first'), ['sex', 'smoker', 'time']),\n",
    "    (Binarizer(threshold=2), ['size']),\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0018c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = make_pipeline(\n",
    "    col_trans,\n",
    "    LinearRegression(),\n",
    ")\n",
    "\n",
    "pl.fit(tips.drop('tip', axis=1), tips['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5db5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question 🤔 (Answer at <a class=\"alert-link\" href=\"http://q.dsc80.com\">q.dsc80.com</a>)</h3>\n",
    "</div>\n",
    "    \n",
    "How many weights does this linear model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598a5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "358b4fd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847fde4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heights and weights\n",
    "\n",
    "We have a dataset containing the weights and heights of 25,000 18 year olds, taken from [here](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e382366",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_path = Path('data') / 'SOCR-HeightWeight.csv'\n",
    "people = pd.read_csv(people_path).drop(columns=['Index'])\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353bbef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "people.plot(kind='scatter', x='Height (Inches)', y='Weight (Pounds)', \n",
    "            title='Weight vs. Height for 25,000 18 Year Olds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d483891c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivating example\n",
    "\n",
    "Suppose we fit a simple linear regression model that uses **height in inches** to predict **weight in pounds**.\n",
    "\n",
    "$$\\text{predicted weight (pounds)} = w_0 + w_1 \\cdot \\text{height (inches)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = people[['Height (Inches)']]\n",
    "y = people['Weight (Pounds)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_one_feat = LinearRegression()\n",
    "lr_one_feat.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc718153",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$w_0^*$ and $w_1^*$ are shown below, along with the model's training set RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f24dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_one_feat.intercept_, lr_one_feat.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ede43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, lr_one_feat.predict(X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6717fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, suppose we fit another regression model, that uses **height in inches** AND **height in centimeters** to predict weight.\n",
    "\n",
    "$$\\text{predicted weight (pounds)} = w_0 + w_1 \\cdot \\text{height (inches)} + w_2 \\cdot \\text{height (cm)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "people['Height (cm)'] = people['Height (Inches)'] * 2.54 # 1 inch = 2.54 cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26777067",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = people[['Height (Inches)', 'Height (cm)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_two_feat = LinearRegression()\n",
    "lr_two_feat.fit(X2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52ada7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What are $w_0^*$, $w_1^*$, $w_2^*$, and the model's test RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_two_feat.intercept_, lr_two_feat.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d37245",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, lr_two_feat.predict(X2), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e9ed4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Observation**: The intercept is the same as before (roughly -82.57), as is the RMSE. However, the coefficients on `'Height (Inches)'` and `'Height (cm)'` are massive in size!\n",
    "\n",
    "What's going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe1e42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Redundant features\n",
    "\n",
    "Let's use simpler numbers for illustration. Suppose in the first model, $w_0^* = -80$ and $w_1^* = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee9e2f",
   "metadata": {},
   "source": [
    "$$\\text{predicted weight (pounds)} = -80 + 3 \\cdot \\text{height (inches)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57392da8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the second model, we have:\n",
    "\n",
    "$$\\begin{align*}\\text{predicted weight (pounds)} &= w_0^* + w_1^* \\cdot \\text{height (inches)} + w_2^* \\cdot \\text{height (cm)} \\\\ &= w_0^* + w_1^* \\cdot \\text{height (inches)} + w_2^* \\cdot \\big( 2.54^* \\cdot \\text{height (inches)} \\big) \\\\ &= w_0^* + \\left(w_1^* + 2.54 \\cdot w_2^* \\right) \\cdot \\text{height (inches)} \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ddd66",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the first model, we already found the \"best\" intercept ($-80$) and slope ($3$) in a linear model that uses height in inches to predict weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28149c8f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**So, as long as $w_1^* + 2.54 \\cdot w_2^* = 3$ in the second model, the second model's training predictions will be the same as the first, and hence they will also minimize RMSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9489bc9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Infinitely many parameter choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e03a59",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Issue**: There are an infinite number of $w_1^*$ and $w_2^*$ that satisfy $w_1^* + 2.54 \\cdot w_2^* = 3$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ea315",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{predicted weight} = -80 - 10 \\cdot \\text{height (inches)} + \\frac{13}{2.54} \\cdot \\text{height (cm)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708169d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{predicted weight} = -80 + 10 \\cdot \\text{height (inches)} - \\frac{7}{2.54} \\cdot \\text{height (cm)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd398c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Both prediction rules look very different, but actually make the same predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b5e83",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `lr.coef_` could return either set of coefficients, or any other of the infinitely many options. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d3015",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But neither set of coefficients is **has any meaning!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247562aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(-80 - 10 * people.iloc[:, 0] + (13 / 2.54) * people.iloc[:, 2]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "(-80 + 10 * people.iloc[:, 0] - (7 / 2.54) * people.iloc[:, 2]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb367e38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6782dc3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multicollinearity occurs when features in a regression model are **highly correlated** with one another.\n",
    "    - In other words, multicollinearity occurs when **a feature can be predicted using a linear combination of other features, fairly accurately**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b361d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When multicollinearity is present in the features, the **coefficients in the model** are uninterpretable – they have no meaning.\n",
    "    - A \"slope\" represents \"the rate of change of $y$ with respect to a feature\", when all other features are held constant – but if there's multicollinearity, you can't hold other features constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd925703",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Note: Multicollinearity doesn't impact a model's predictions!**\n",
    "    - It doesn't impact a model's ability to generalize to unseen data.\n",
    "    - If features are multicollinear in the training data, they will probably be multicollinear in the test data too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f36b83",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Solutions**:\n",
    "    - Manually remove highly correlated features.\n",
    "    - Use a dimensionality reduction technique (such as PCA) to automatically reduce dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caacea60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: One hot encoding\n",
    "\n",
    "**A one hot encoding will result in multicollinearity unless you drop one of the one hot encoded features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da2ba8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have the following fitted model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(x) = 1 + 2 \\cdot (\\text{smoker==Yes}) - 2 \\cdot (\\text{smoker==No})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa205b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(x) = 10 - 7 \\cdot (\\text{smoker==Yes}) - 11 \\cdot (\\text{smoker==No})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a1914",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Solution: Drop one of the one hot encoded columns. `sklearn.preprocessing.OneHotEncoder` has an option to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8f4b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06fdef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multicollinearity is present in a linear model when one feature can be accurately predicted using one or more other features.\n",
    "    - In other words, it is present when a feature is **redundant**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd480761",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multicollinearity doesn't pose an issue for prediction; it doesn't hinder a model's ability to generalize. Instead, it renders the **coefficients** of a linear model meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c0fc4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question 🤔 (Answer at <a class=\"alert-link\" href=\"http://q.dsc80.com\">q.dsc80.com</a>)</h3>\n",
    "</div>\n",
    "\n",
    "(Wi23 Final Q9)\n",
    "\n",
    "One piece of information that may be useful as a feature is the proportion of SAT test takers in a state in a given year that qualify for free lunches in school. The Series `lunch_props` contains 8 values, each of which are either `\"low\"`, `\"medium\"`, or `\"high\"`. Since we can’t use strings as features in a model, we decide to encode these strings using the following Pipeline:\n",
    "\n",
    "```python\n",
    "# Note: The FunctionTransformer is only needed to change the result\n",
    "# of the OneHotEncoder from a \"sparse\" matrix to a regular matrix\n",
    "# so that it can be used with StandardScaler;\n",
    "# it doesn't change anything mathematically.\n",
    "pl = Pipeline([\n",
    "    (\"ohe\", OneHotEncoder(drop=\"first\")),\n",
    "    (\"ft\", FunctionTransformer(lambda X: X.toarray())),\n",
    "    (\"ss\", StandardScaler())\n",
    "])\n",
    "```\n",
    "\n",
    "After calling `pl.fit(lunch_props)`, `pl.transform(lunch_props)` evaluates to the following array:\n",
    "\n",
    "```python\n",
    "array([[ 1.29099445, -0.37796447],\n",
    "       [-0.77459667, -0.37796447],\n",
    "       [-0.77459667, -0.37796447],\n",
    "       [-0.77459667,  2.64575131],\n",
    "       [ 1.29099445, -0.37796447],\n",
    "       [ 1.29099445, -0.37796447],\n",
    "       [-0.77459667, -0.37796447],\n",
    "       [-0.77459667, -0.37796447]])\n",
    "```\n",
    "\n",
    "and `pl.named_steps[\"ohe\"].get_feature_names()` evaluates to the following array:\n",
    "\n",
    "```python\n",
    "array([\"x0_low\", \"x0_med\"], dtype=object)\n",
    "```\n",
    "\n",
    "Fill in the blanks: Given the above information, we can conclude that lunch_props has ____________ value(s) equal to \"low\", ____________ value(s) equal to \"medium\", and _____________ value(s) equal to \"high\". (Note: You should write one positive integer in each box such that the numbers add up to 8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2cdae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f29f107",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b515aed4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906265c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You and Billy are studying for an upcoming exam. You both decide to test your understanding by taking a **practice exam**.\n",
    "    - Your logic: If you do well on the practice exam, you should do well on the real exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d060d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You each take the practice exam once and look at the solutions afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f457b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Your strategy**: Memorize the answers to all practice exam questions, e.g. \"Question 1: A; Question 2: C; Question 3: A.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ad577",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Billy's strategy**: Learn high-level concepts from the solutions, e.g. \"data are NMAR if the likelihood of missingness depends on the missing values themselves.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26344c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Who will do better on the **practice exam**? Who will probably do better on the **real exam**? 🧐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a59976",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating the quality of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2051d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far, we've computed the RMSE (and $R^2$) of our fit regression models on the **data that we used to fit them**, i.e. the **training data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a762c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We've said that Model A is **better** than Model B if Model A's RMSE is **lower** than Model B's RMSE.\n",
    "    - Remember, our **training data** is a sample from some population.\n",
    "    - Just because a model fits the training data well doesn't mean it will **generalize** and work well on **similar, unseen samples** from the same population!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c467ec3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Overfitting and underfitting\n",
    "\n",
    "Let's collect two samples $\\{(x_i, y_i)\\}$ from the same population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23) # For reproducibility.\n",
    "\n",
    "def sample_from_pop(n=100):\n",
    "    x = np.linspace(-2, 3, n)\n",
    "    y = x ** 3 + (np.random.normal(0, 3, size=n))\n",
    "    return pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "sample_1 = sample_from_pop()\n",
    "sample_2 = sample_from_pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f617db",
   "metadata": {},
   "source": [
    "For now, let's just look at Sample 1. The relationship between $x$ and $y$ is roughly **cubic**; that is, $y \\approx x^3$ (remember, in reality, you won't get to see the population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca2247",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "px.scatter(sample_1, x='x', y='y', title='Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c9c50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial regression\n",
    "\n",
    "Let's fit three **polynomial** models on Sample 1:\n",
    "- Degree 1.\n",
    "- Degree 3.\n",
    "- Degree 25.\n",
    "\n",
    "The `PolynomialFeatures` transformer will be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cecc330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform fits and transforms the same input.\n",
    "d2 = PolynomialFeatures(3)\n",
    "d2.fit_transform(np.array([1, 2, 3, 4, -2]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e37f18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below, we look at our three models' predictions on Sample 1 (which they were trained on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the definition of train_and_plot in lec15_util.py if you're curious as to how the plotting works.\n",
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_1, degs=[1, 3, 25], data_name='Sample 1')\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f925dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The degree 25 polynomial has the lowest RMSE on Sample 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3caba92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do the same fit polynomials look on Sample 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea23c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_2, degs=[1, 3, 25], data_name='Sample 2')\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce2ddd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The degree 3 polynomial has the lowest RMSE on Sample 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f9729",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that **we didn't get to see Sample 2 when fitting our models**! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6081b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, it seems that the degree 3 polynomial **generalizes better** to unseen data than the degree 25 polynomial does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba53e71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we fit a degree 1, degree 3, and degree 25 polynomial **on Sample 2** as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_multiple_models(sample_1, sample_2, degs=[1, 3, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a9155",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key idea**: Degree 25 polynomials seem to **vary more when trained on different samples** than degree 3 and 1 polynomials do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6b397",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias and variance\n",
    "\n",
    "The training data we have access to is a sample from the population. We are concerned with our model's ability to **generalize** and work well on **different datasets** drawn from the same population.\n",
    "\n",
    "Suppose we **fit** a model $H$ (e.g. a degree 3 polynomial) on **several different datasets** from a population. There are three sources of error that arise:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd7a82a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ⭐️ **Bias**: **The expected deviation between a predicted value and an actual value**.\n",
    "    - In other words, **for a given $x_i$, how far is $H(x_i)$ from the true $y_i$, on average?**\n",
    "    - Low bias is good! ✅\n",
    "    - High bias is a sign of **underfitting**, i.e. that our model is too **basic** to capture the relationship between our features and response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ec453f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ⭐️ **Model variance (\"variance\")**: **The variance of a model's predictions**.\n",
    "    - In other words, **for a given $x_i$, how much does $H(x_i)$ vary across all datasets**?\n",
    "    - Low model variance is good! ✅\n",
    "    - High model variance is a sign of **overfitting**, i.e. that our model is too **complicated** and is prone to fitting to the noise in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e16e67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Observation error**: The error due to the random noise in the process we are trying to model (e.g. measurement error). _We can't control this, without collecting more data!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7857fa7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, suppose:\n",
    "- The <span style='color:#c6283f'><b>red bulls-eye</b></span> represents your **true weight and height** 🧍.\n",
    "- The <span style='color:#080c6f'><b>dark blue darts</b></span> represent **predictions of your weight and height** using different models that were fit on the same DGP. \n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/image_5.png\" width=\"40%\"></center>\n",
    "\n",
    "We'd like our models to be in the top left, but in practice that's hard to achieve!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15383fc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Risk vs. empirical risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c4a41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In DSC 40A, we started using **empirical risk minimization** to find optimal model parameters $w^*$:\n",
    "\n",
    "$$\n",
    "\\text{choose the $w$ such that } \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - H(x_i) \\right)^2 \\text{ is minimized}$$\n",
    "\n",
    "<center>or, equivalently:</center>\n",
    "\n",
    "$$w^* = \\underset{w}{\\text{argmin}} \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - H(x_i) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e3894",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea**: A model that works well on past data should work well on future data, if future data looks like past data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65db6d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What we really want is for the **expected loss for a new data point $(x_{\\text{new}}, y_{\\text{new}})$, drawn from the same population as the training set, to be small**. That is, we want\n",
    "    $$\\mathbb{E}[y_{\\text{new}} - H(x_{\\text{new}})]^2$$\n",
    "    to be minimized. The quantity above is called **risk**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8ca84",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What's that fancy $\\mathbb{E}$? It is the **expectation** operator of a random variable: it computes the **average value** of the random variable across its entire distribution.\n",
    "    - (From Math 183/180A) If $X \\sim \\text{Binomial}(n, p)$, then $\\mathbb{E}[X] = np$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4505e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In general, we don't know the entire population distribution of $x$s and $y$s, so we can't compute risk exactly. That's why we compute **empirical risk**!\n",
    "\n",
    "$$\\mathbb{E}[y_{\\text{new}} - H(x_{\\text{new}})]^2 \\approx \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - H(x_i) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce39c78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The bias-variance decomposition\n",
    "\n",
    "Risk can be decomposed as follows:\n",
    "\n",
    "$$\\mathbb{E}[y_{\\text{new}} - H(x_{\\text{new}})]^2 = \\text{model bias}^2 + \\text{model variance} + \\text{observation error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38435d86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember, this expectation $\\mathbb{E}$ is over the entire population of $x$s and $y$s: in real life, we don't know what this population distribution is, so we can't put actual numbers to this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd04bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $H$ is too simple to capture the relationship between $x$s and $y$s in the population, $H$ will **underfit** to training sets and have **high bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe25143b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $H$ is overly complex, $H$ will **overfit** to training sets and have **high variance**, meaning it will change significantly from one training set to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc06760",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Generally:\n",
    "    - Training error reflects bias, **not variance**.\n",
    "    - Test error reflects **both bias and variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb3f02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Navigating the bias-variance tradeoff\n",
    "\n",
    "$$\\mathbb{E}[y_{\\text{new}} - H(x_{\\text{new}})]^2 = \\text{model bias}^2 + \\text{model variance} + \\text{observation error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ac6aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we collect more data points (i.e. as $n \\uparrow$):\n",
    "    - Model variance decreases.\n",
    "    - If $H$ can exactly model the true population relationship between $x$ and $y$ (e.g. cubic), then model bias also decreases.\n",
    "    - If $H$ can't exactly model the true population relationship between $x$ and $y$, then model bias will remain large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4fff9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we add more features (i.e. as $d \\uparrow$):\n",
    "    - Model variance increases, whether or not the feature was useful.\n",
    "    - Adding a useful feature decreases model bias.\n",
    "    - Adding a useless feature doesn't change model bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c261d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: suppose the actual relationship between $x$ and $y$ in the population is linear, and we fit $H$ using simple linear regression.\n",
    "    - Model bias = 0.\n",
    "    - Model variance $\\propto \\frac{d}{n}$.\n",
    "        - As $d \\uparrow$, model variance $\\uparrow$.\n",
    "        - As $n \\uparrow$, model variance $\\downarrow$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5276ca6",
   "metadata": {},
   "source": [
    "Read more [here](https://learningds.org/ch/17/inf_pred_gen_prob.html#probability-behind-model-selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c004c65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question 🤔 (Answer at <a class=\"alert-link\" href=\"http://q.dsc80.com\">q.dsc80.com</a>)</h3>\n",
    "</div>\n",
    "\n",
    "(Fa23 Final 9.4)\n",
    "\n",
    "Determine how each change below affects model bias and variance compared to this model:\n",
    "\n",
    "<center><img src='imgs/hx.png' width=50%></center>\n",
    "\n",
    "For each change, choose all of the following that apply: **increase bias, decrease bias, increase variance, decrease variance.**\n",
    "\n",
    "1. Add degree 3 polynomial features.\n",
    "1. Add a feature of numbers chosen at random between 0 and 1.\n",
    "1. Collect 100 more points for the training set.\n",
    "1. Don’t use the 'veg' feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c8997",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train-test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa9209",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Avoiding overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e79f8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We won't know whether our model has **overfit** to our sample (training data) unless we get to see how well it performs on a new sample from the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba738e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 💡**Idea**: **Split** our sample into a **training set** and **test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055abc38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use **only** the training set to fit the model (i.e. find $w^*$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f695a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use the test set to evaluate the model's error (RMSE, $R^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbca066",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The test set is like a new sample of data from the same population as the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa41550",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/train-test.png\" width='50%'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f6da3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train-test split 🚆\n",
    "\n",
    "`sklearn.model_selection.train_test_split` implements a train-test split for us! 🙏🏼 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5e8c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If `X` is an array/DataFrame of features and `y` is an array/Series of responses,\n",
    "\n",
    "```py\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "```\n",
    "\n",
    "randomly splits the features and responses into training and test sets, such that the test set contains 0.25 of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9f895",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the documentation!\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf97327",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's perform a train/test split on our `tips` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0abdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips.drop('tip', axis=1)\n",
    "y = tips['tip']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # We don't have to choose 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e98b61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before proceeding, let's check the sizes of `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows in X_train:', X_train.shape[0])\n",
    "display(X_train.head())\n",
    "print('Rows in X_test:', X_test.shape[0])\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f2b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape[0] / tips.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204fc207",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example train-test split\n",
    "\n",
    "Steps:\n",
    "1. Fit a model on the training set.\n",
    "2. Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips[['total_bill', 'size']] # For this example, we'll use just the already-quantitative columns in tips.\n",
    "y = tips['tip']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # random_state is like np.random.seed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e38d01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, we'll use a stand-alone `LinearRegression` model without a `Pipeline`, but this process would work the same if we were using a `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9219685",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's check our model's performance on the **training** set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18894cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = lr.predict(X_train)\n",
    "rmse_train = mean_squared_error(y_train, pred_train, squared=False)\n",
    "rmse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20d459",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And the **test** set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = lr.predict(X_test)\n",
    "rmse_test = mean_squared_error(y_test, pred_test, squared=False)\n",
    "rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c22c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since `rmse_train` and `rmse_test` are similar, it **doesn't seem like our model is overfitting** to the training data. If `rmse_test` was much larger than `rmse_train`, it would be evidence that our model is unable to **generalize well**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b176323a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b338d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Polynomial regression\n",
    "\n",
    "We recently looked at an example of **polynomial regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_2, degs=[1, 3, 25], data_name='Sample 2')\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a671d7",
   "metadata": {},
   "source": [
    "When building these models:\n",
    "- We **got to choose** the degree of the polynomials (i.e. we chose 1, 3, and 25).\n",
    "- We didn't get to choose the exact formulas for the three polynomials – their formulas were **learned from data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0e314",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters vs. hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d7d0c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **parameter** defines the relationship between variables in a model. \n",
    "    - **We learn parameters from data**.\n",
    "    - For instance, suppose we fit a degree 3 polynomial to data, and end up with\n",
    "    \n",
    "    $$H(x) = 1 - 2x + 13x^2 - 4x^3$$\n",
    "    \n",
    "    - 1, -2, 13, and -4 are parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778dd62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **hyperparameter** is a parameter that we get to choose **before our model is fit to the data**.\n",
    "    - Think of hyperparameters as knobs 🎛 – **we get to pick and tune them!**\n",
    "    - **Polynomial degree** was a hyperparameter in the previous example, and we tried three different values: 1, 3, and 25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dfbc3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we choose the \"right\" hyperparameter(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f97b99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. test error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2e40f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We know that a model's performance on a **test set** is a good estimate of its ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bc6c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We want to find the hyperparameter that leads to the best **test set performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e28be9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Idea:\n",
    "    1. Come up with a **list** of hyperparameters to try.\n",
    "    2. For each hyperparameter, train the model on the training set and compute its performance on the test set.\n",
    "    3. Pick the hyperparameter with the best performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31001a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. test error\n",
    "\n",
    "- Let's try this strategy on Sample 1 from our earlier example. \n",
    "\n",
    "- We'll try to fit a polynomial model on the dataset; we'll choose the polynomial's degree from the list [1, 2, ..., 25]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(sample_1, x='x', y='y', title='Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c1eb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, we perform a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_1[['x']]\n",
    "y = sample_1['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ff996",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial degree vs. train/test error\n",
    "\n",
    "Now, we'll create models with degree-1 through degree-25 polynomial features and compute their train and test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errs = []\n",
    "test_errs = []\n",
    "\n",
    "for d in range(1, 26):\n",
    "    pl = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "    pl.fit(X_train, y_train)\n",
    "    train_errs.append(mean_squared_error(y_train, pl.predict(X_train), squared=False))\n",
    "    test_errs.append(mean_squared_error(y_test, pl.predict(X_test), squared=False))\n",
    "\n",
    "errs = pd.DataFrame({'Train Error': train_errs, 'Test Error': test_errs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06ac65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the plots of training error vs. degree and test error vs. degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(errs)\n",
    "fig.update_layout(showlegend=True, xaxis_title='Polynomial Degree', yaxis_title='RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128dfbf6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training error appears to decrease as polynomial degree increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724335d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Test error appears to decrease until a \"valley\", and then increases again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e0bee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we'd choose a degree of 3, since that degree has the **lowest test error**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0abe896",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. test error\n",
    "\n",
    "The pattern we saw in the previous example is true more generally.\n",
    "\n",
    "<center><img src='imgs/tt-errors.png' width=50%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728db66",
   "metadata": {},
   "source": [
    "We pick the hyperparameter(s) at the \"valley\" of test error.\n",
    "\n",
    "Note that training error **tends** to underestimate test error, but it doesn't have to – i.e., it is possible for test error to be lower than training error (say, if the test set is \"easier\" to predict than the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6bc281",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conducting train-test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88811055",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall, <span style='color: blue'><b>training data</b></span> is used to fit our model, and <span style='color: orange'><b>test data</b></span> is used to evaluate our model.\n",
    "\n",
    "<center><img src='imgs/train-test-first.png' width=40%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc6b2ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: _How_ should we split?\n",
    "    - `sklearn`'s `train_test_split` splits **randomly**, which usually works well.\n",
    "    - However, if there is some element of **time** in the training data (say, when predicting the future price of a stock), a better split is \"past\" and \"future\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec1df4b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How _large_ should the split be, e.g. 90%-10% vs. 75%-25%?\n",
    "    - There's a tradeoff – a larger training set should lead to a \"better\" model, while a larger test set should lead to a better estimate of our model's ability to generalize.\n",
    "    - There's no \"right\" choice, but we usually choose between 10% to 25% for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf0394",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### But wait..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c19ac8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- With our current strategy, we are choosing the hyperparameter that creates the model that **performs best on the test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427cc42",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, we are **overfitting to the test set** – the best hyperparameter for the test set might not be the best hyperparameter for a totally unseen dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01584eef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems like we need **another** split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa13de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4934626",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- We want to build models that generalize well to unseen data.\n",
    "    - Models that have **high bias** are **too simple** to represent complex relationships in data, and **underfit**.\n",
    "    - Models that have **high variance** are **overly complex** for the relationships in the data, and vary a lot when fit on different datasets. Such models **overfit** to the training data.\n",
    "- A model's training error tends to decrease as model complexity increases, while its test error tends to decrease, before reaching a \"sweet spot\" and increasing again.\n",
    "- A hyperparameter is a configuration that we choose before training a model; an important task in machine learning is selecting \"good\" hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9c25c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next time\n",
    "\n",
    "We can't choose model hyperparameters just by using a train-test split, because this strategy overfits to the test data.\n",
    "\n",
    "Is there a better way to choose model hyperparameters? **Yes: cross-validation**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
