{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8a44b8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from dsc80_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e66d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5 â€“ Exploratory Data Analysis and Data Cleaning\n",
    "\n",
    "## DSC 80, Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22824200",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements ðŸ“£\n",
    "\n",
    "- Lab 2 due tomorrow, **Wed, April 17**.\n",
    "- Project 1 is due this **Fri, April 19**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd851d5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda ðŸ“†\n",
    "\n",
    "- Other data representations.\n",
    "- Dataset overview.\n",
    "- Introduction to `plotly`.\n",
    "- Exploratory data analysis and feature types.\n",
    "- Data cleaning.\n",
    "    - Data quality checks.\n",
    "    - Missing values.\n",
    "    - Transformations and timestamps.\n",
    "    - Modifying structure.\n",
    "- Investigating student-submitted questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56a0ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a href=\"https://q.dsc80.com\">q.dsc80.com)</h3>\n",
    "</div>\n",
    "    \n",
    "Remember, you can always ask questions at [**q.dsc80.com**](https://q.dsc80.com)! If the link doesn't work for you, click the [**ðŸ¤” Lecture Questions**](https://docs.google.com/forms/d/e/1FAIpQLScWbVZv9hBv-wX-ItKHUVRnkPMMtfJZVfErKE9GS7_8dFcRBQ/viewform) link in the top right corner of the course website.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed6c3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5d3b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### San Diego food safety\n",
    "\n",
    "From [this article](https://inewsource.org/2023/02/09/san-diego-restaurants-food-safety-violations/) ([archive link](https://archive.ph/gz8BL)):\n",
    "\n",
    "> In the last three years, one third of San Diego County restaurants have had at least one major food safety violation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a6782",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 99% Of San Diego Restaurants Earn â€˜A' Grades, Bringing Usefulness of System Into Question\n",
    "\n",
    "From [this article](https://www.nbcsandiego.com/news/local/99-of-san-diego-restaurants-earn-a-grades-bringing-usefulness-of-system-into-question/25381/) ([archive link](https://archive.ph/yB6RU)):\n",
    "\n",
    "> Food held at unsafe temperatures. Employees not washing their hands. Dirty countertops. Vermin in the kitchen. An expired restaurant permit.\n",
    "> \n",
    "> Restaurant inspectors for San Diego County found these violations during a routine health inspection of a diner in La Mesa in November 2016. Despite the violations, the restaurant was awarded a score of 90 out of 100, the lowest possible score to achieve an â€˜Aâ€™ grade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac10042",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The data\n",
    "\n",
    "- We downloaded the data about the 1000 restaurants closest to UCSD from [here](https://www.sandiegocounty.gov/content/sdc/deh/fhd/ffis/intro.html.html).\n",
    "- We had to download the data as JSON files, then process it into DataFrames. You'll learn how to do this soon!\n",
    "    - Until now, you've (largely) been presented with CSV files that `pd.read_csv` could load without any issues.\n",
    "    - But there are many different formats and possible issues when loading data in from files.\n",
    "    - See [Chapter 8 of Learning DS](https://learningds.org/ch/08/files_intro.html) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_path = Path('data') / 'restaurants.csv'\n",
    "insp_path = Path('data') / 'inspections.csv'\n",
    "viol_path = Path('data') / 'violations.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = pd.read_csv(rest_path)\n",
    "insp = pd.read_csv(insp_path)\n",
    "viol = pd.read_csv(viol_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5db5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a class=\"alert-link\" href=\"http://q.dsc80.com\">q.dsc80.com)</h3>\n",
    "</div>\n",
    "    \n",
    "The first article said that one third of restaurants had at least one major safety violation.<br>\n",
    "Which DataFrames and columns seem most useful to verify this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8367ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb951ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf62cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "viol.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b64207",
   "metadata": {},
   "outputs": [],
   "source": [
    "viol.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8cbd01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction to `plotly`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f270da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `plotly`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba464496",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We've used `plotly` in lecture briefly, and you even have to use it in Project 1 Question 13, but we haven't yet discussed it formally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d895a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It's a visualization library that enables **interactive** visualizations.\n",
    "\n",
    "<center><img src=\"imgs/plotly.png\" width=40%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d9f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `plotly`\n",
    "\n",
    "There are a few ways we can use `plotly`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622401c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using the `plotly.express` syntax.\n",
    "    - `plotly` is very flexible, but it can be verbose; `plotly.express` allows us to make plots quickly.\n",
    "    - See the [**documentation here**](https://plotly.com/python/plotly-express) â€“ it's very rich (there are good examples for almost everything)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d84013",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By setting `pandas` plotting backend to `'plotly'` (by default, it's `'matplotlib'`) and using the DataFrame `plot` method.\n",
    "    - The DataFrame `plot` method is how you created plots in DSC 10!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd73d31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For now, we'll use `plotly.express` syntax; we've imported it in the `dsc80_utils.py` file that we import at the top of each lecture notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a04b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initial plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85364a84",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's look at the distribution of inspection `'score'`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74310df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(insp['score'])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de6973",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about the distribution of average inspection `'score'` per `'grade'`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = (\n",
    "    insp[['grade', 'score']]\n",
    "    .dropna()\n",
    "    .groupby('grade')\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "# x= and y= are columns of scores. Convenient!\n",
    "px.bar(scores, x='grade', y='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b951fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as the above!\n",
    "scores.plot(kind='bar', x='grade', y='score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16546d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploratory data analysis and feature types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50511216",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The data science lifecycle, revisited\n",
    "\n",
    "<center>\n",
    "    <img src=\"imgs/ds-lifecycle.svg\" width=50%>\n",
    "</center>\n",
    "\n",
    "We're at the stage of **understanding the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4dcfca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploratory data analysis (EDA)\n",
    "\n",
    "- Historically, data analysis was dominated by formal statistics, including tools like confidence intervals, hypothesis tests, and statistical modeling.\n",
    "\n",
    "- In 1977, John Tukey [defined](https://search.worldcat.org/title/3058187) the term **exploratory data analysis**, which described a philosophy for proceeding about data analysis:\n",
    "\n",
    "> Exploratory data analysis is actively incisive, rather than passively descriptive, with real emphasis on the discovery of the unexpected.\n",
    "\n",
    "- Practically, EDA involves, among other things, computing summary statistics and drawing plots to understand the nature of the data at hand.\n",
    "\n",
    "> The greatest gains from data come from surprisesâ€¦ The unexpected is best brought to our attention by **pictures**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41ad00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Different feature types\n",
    "\n",
    "<center><img src='imgs/data-types.png' width=90%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7696f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a class=\"alert-link\" href=\"http://q.dsc80.com\">q.dsc80.com)</h3>\n",
    "        \n",
    "Determine the <b>feature type</b> of each of the following variables.\n",
    "    \n",
    "- `insp['score']`\n",
    "- `insp['grade']`\n",
    "- `viol['violation_accela']`\n",
    "- `viol['major_violation']`\n",
    "- `rest['business_id']`\n",
    "- `rest['opened_date']`\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252004ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32888d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature types vs. data types\n",
    "\n",
    "- The data type `pandas` uses is not the same as the \"data type\" we talked about just now!\n",
    "    - There's a difference between feature type and computational data type.\n",
    "\n",
    "- Take care when the two don't match up very well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dab3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas stores these as ints, but they're actually nominal.\n",
    "rest['business_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas stores these as strings, but they're actually numeric.\n",
    "rest['opened_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fdcc40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e1505",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Four pillars of data cleaning\n",
    "\n",
    "When loading in a dataset, to clean the data â€“ that is, to prepare it for further analysis â€“ we will:\n",
    "\n",
    "1. Perform **data quality checks**.\n",
    "\n",
    "2. Identify and handle **missing values**.\n",
    "\n",
    "3. Perform **transformations**, including converting time series data to **timestamps**.\n",
    "\n",
    "4. Modify **structure** as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8786b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data cleaning: Data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92564b1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data quality checks\n",
    "\n",
    "We often start an analysis by checking the quality of the data.\n",
    "\n",
    "- Scope: Do the data match your understanding of the population? \n",
    "- Measurements and values: Are the values reasonable?\n",
    "- Relationships: Are related features in agreement?\n",
    "- Analysis: Which features might be useful in a future analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdeb3ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scope\n",
    "\n",
    "Do the data match your understanding of the population?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ecab4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We were told that we're only looking at the 1000 restaurants closest to UCSD, so the restaurants in `rest` should agree with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef21b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df0230",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measurements and values\n",
    "\n",
    "Are the values reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c5d81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Do the values in the `'grade'` column match what we'd expect grades to look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce83dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp['grade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72806b8d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What kinds of information does the `insp` DataFrame hold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3206cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f33640",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What's going on in the `'address'` column of `rest`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there multiple restaurants with the same address?\n",
    "rest['address'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeps all rows with duplicate addresses.\n",
    "(\n",
    "    rest\n",
    "    .groupby('address')\n",
    "    .filter(lambda df: df.shape[0] >= 2)\n",
    "    .sort_values('address')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ace0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the same thing as above!\n",
    "(\n",
    "    rest[rest.duplicated(subset=['address'], keep=False)]\n",
    "    .sort_values('address')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773936b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relationships\n",
    "\n",
    "Are related features in agreement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4836c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Do the `'address'`es and `'zip'` codes in `rest` match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4895b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest[['address', 'zip']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e8838b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What about the `'score'`s and `'grade'`s in `insp`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp[['score', 'grade']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7a3c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analysis\n",
    "\n",
    "Which features might be useful in a future analysis?\n",
    "\n",
    "- We're most interested in:\n",
    "    - These columns in the `rest` DataFrame: `'business_id'`, `'name'`, `'address'`, `'zip'`, and `'opened_date'`.\n",
    "    - These columns in the `insp` DataFrame: `'business_id'`, `'inspection_id'`, `'score'`, `'grade'`, `'completed_date'`, and `'status'`.\n",
    "    - These columns in the `viol` DataFrame: `'inspection_id'`, `'violation'`, `'major_violation'`, `'violation_text'`, and `'violation_accela'`.\n",
    "\n",
    "- Also, let's rename a few columns to make them easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26694343",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ðŸ’¡ Pro-Tip: Using `pipe`\n",
    "\n",
    "When we manipulate DataFrames, it's best to define individual functions for each step, then use the `pipe` **method** to chain them all together.\n",
    "\n",
    "The `pipe` DataFrame method takes in a function, which itself takes in a DataFrame and returns a DataFrame.\n",
    "\n",
    "- In practice, we would add functions one by one to the top of a notebook, then `pipe` them all.\n",
    "- For today, will keep re-running `pipe` to show data cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_rest(rest):\n",
    "    return rest[['business_id', 'name', 'address', 'zip', 'opened_date']]\n",
    "\n",
    "rest = (\n",
    "    pd.read_csv(rest_path)\n",
    "    .pipe(subset_rest)\n",
    ")\n",
    "rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2580209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as the above â€“ but the above makes it easier to chain more .pipe calls afterwards.\n",
    "subset_rest(pd.read_csv(rest_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e751e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's use `pipe` to keep (and rename) the subset of the columns we care about in the other two DataFrames as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_insp(insp):\n",
    "    return (\n",
    "        insp[['business_id', 'inspection_id', 'score', 'grade', 'completed_date', 'status']]\n",
    "        .rename(columns={'completed_date': 'date'})\n",
    "    )\n",
    "\n",
    "insp = (\n",
    "    pd.read_csv(insp_path)\n",
    "    .pipe(subset_insp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_viol(viol):\n",
    "    return (\n",
    "        viol[['inspection_id', 'violation', 'major_violation', 'violation_accela']]\n",
    "        .rename(columns={'violation': 'kind',\n",
    "                         'major_violation': 'is_major',\n",
    "                         'violation_accela': 'violation'})\n",
    "    )\n",
    "\n",
    "viol = (\n",
    "    pd.read_csv(viol_path)\n",
    "    .pipe(subset_viol)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fd7f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Combining the restaurant data\n",
    "\n",
    "Let's join all three DataFrames together so that we have all the data in a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ca1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_restaurant_data():\n",
    "    return (\n",
    "        rest\n",
    "        .merge(insp, on='business_id', how='left')\n",
    "        .merge(viol, on='inspection_id', how='left')\n",
    "    )\n",
    "\n",
    "df = merge_all_restaurant_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d2a9f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a class=\"alert-link\" href=\"http://q.dsc80.com\">q.dsc80.com)</h3>\n",
    "\n",
    "Why should the function above use two left joins? What would go wrong if we used other kinds of joins?\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92823e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data cleaning: Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007e6ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Missing values\n",
    "\n",
    "Next, it's important to check for and handle missing values, as they can have a big effect on your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8190fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp[['score', 'grade']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proportion of values in each column that are missing.\n",
    "insp.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fdee4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Why are there null values here?\n",
    "# insp['inspection_id'] and viol['inspection_id'] don't have any null values...\n",
    "df[df['inspection_id'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afddb353",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are many ways of handling missing values, which we'll cover in an entire lecture next week. But a good first step is to check how many there are!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f39cbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data cleaning: Transformations and timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce00205",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformations and timestamps\n",
    "\n",
    "From last class:\n",
    "\n",
    "> A transformation results from performing some operation on every element in a sequence, e.g. a Series.\n",
    "\n",
    "It's often useful to look at ways of transforming your data to make it easier to work with.\n",
    "\n",
    "- Type conversions (e.g. changing the string `\"$2.99\"` to the number `2.99`).\n",
    "\n",
    "- Unit conversion (e.g. feet to meters).\n",
    "\n",
    "- Extraction (Getting `'vermin'` out of `'Vermin Violation Recorded on 10/10/2023'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87244b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating timestamps\n",
    "\n",
    "Most commonly, we'll parse dates into `pd.Timestamp` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the dtype!\n",
    "insp['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adaa896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This magical string tells Python what format the date is in.\n",
    "# For more info: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "date_format = '%Y-%m-%d'\n",
    "pd.to_datetime(insp['date'], format=date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54dee4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Another advantage of defining functions is that we can reuse this function\n",
    "# for the 'opened_date' column in `rest` if we wanted to.\n",
    "def parse_dates(insp, col):\n",
    "    date_format = '%Y-%m-%d'\n",
    "    dates = pd.to_datetime(insp[col], format=date_format)\n",
    "    return insp.assign(**{col: dates})\n",
    "\n",
    "insp = (\n",
    "    pd.read_csv(insp_path)\n",
    "    .pipe(subset_insp)\n",
    "    .pipe(parse_dates, 'date')\n",
    ")\n",
    "\n",
    "# We should also remake df, since it depends on insp.\n",
    "# Note that the new insp is used to create df!\n",
    "df = merge_all_restaurant_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the dtype now!\n",
    "df['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9739b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Working with timestamps\n",
    "\n",
    "- We often want to adjust granularity of timestamps to see overall trends, or seasonality.\n",
    "- Use the `resample` method in `pandas` ([documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects)).\n",
    "    - Think of it like a version of `groupby`, but for timestamps.\n",
    "    - For instance, `insp.resample('2W', on='date')` separates every two weeks of data into a different group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp.resample('2W', on='date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are those numbers coming from?\n",
    "insp[\n",
    "    (insp['date'] >= pd.Timestamp('2020-01-05')) &\n",
    "    (insp['date'] < pd.Timestamp('2020-01-19'))\n",
    "]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d55148",
   "metadata": {},
   "outputs": [],
   "source": [
    "(insp.resample('2W', on='date')\n",
    " .size()\n",
    " .plot(title='Number of Inspections Over Time')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622796b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The `.dt` accessor\n",
    "\n",
    "Like with Series of strings, `pandas` has a `.dt` accessor for properties of timestamps ([documentation](https://pandas.pydata.org/docs/user_guide/basics.html#basics-dt-accessors))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3cf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14818dc0",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "insp['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71848903",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "insp['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ae178",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "dow_counts = insp['date'].dt.dayofweek.value_counts()\n",
    "fig = px.bar(dow_counts)\n",
    "fig.update_xaxes(tickvals=np.arange(7), ticktext=['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206c9b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data cleaning: Modifying structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ddb97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reshaping DataFrames\n",
    "\n",
    "We often **reshape** the DataFrame's structure to make it more convenient for analysis. For example, we can:\n",
    "\n",
    "- Simplify structure by removing columns or taking a set of rows for a particular period of time or geographic area.\n",
    "    - We already did this!\n",
    "\n",
    "- Adjust granularity by aggregating rows together.\n",
    "    - To do this, use `groupby` (or `resample`, if working with timestamps).\n",
    "\n",
    "- Reshape structure, most commonly by using the DataFrame `melt` method to un-pivot a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e99cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `melt`\n",
    "\n",
    "- The `melt` method is common enough that we'll give it a special mention.\n",
    "- We'll often encounter pivot tables (esp. from government data), which we call *wide* data.\n",
    "- The methods we've introduced work better with *long-form* data, or *tidy* data.\n",
    "- To go from wide to long, `melt`.\n",
    "\n",
    "<center><img src='imgs/wide-vs-long.svg' width=40%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e43c42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example usage of `melt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54edf07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_example = pd.DataFrame({\n",
    "    'Year': [2001, 2002],\n",
    "    'Jan': [10, 130],\n",
    "    'Feb': [20, 200],\n",
    "    'Mar': [30, 340]\n",
    "}).set_index('Year')\n",
    "wide_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc75510",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_example.melt(ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77837e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67478028",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a href=\"https://q.dsc80.com\">q.dsc80.com)</h3>\n",
    "</div>\n",
    "    \n",
    "What questions do you want me to try and answer with the data? I'll start with a single pre-prepared question, and then answer student questions until we run out of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b53b97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example question: Can we rank restaurants by their number of violations? How about separately for each zip code?\n",
    "\n",
    "And why would we want to do that? ðŸ¤”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df988966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "170d5d0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd620d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Data cleaning is a necessary starting step in data analysis. There are four pillars of data cleaning:\n",
    "    - Quality checks.\n",
    "    - Missing values.\n",
    "    - Transformations and timestamps.\n",
    "    - Modifying structure.\n",
    "- Approach EDA with an open mind, and draw lots of visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8419fc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next time\n",
    "\n",
    "Hypothesis and permutation testing. Some of this will be DSC 10 review, but we'll also push further! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
